# SilentÂ Killers  
### An Exploratory Audit of Exceptionâ€‘Handling in LLMâ€‘Generated Python
![CI](https://github.com/kilojoules/silent-killers/actions/workflows/ci.yml/badge.svg)
![license](https://img.shields.io/badge/license-MIT-blue)
[![PyPI](https://img.shields.io/pypi/v/silent-killers.svg)](https://pypi.org/project/silent-killers/)

> **tl;dr**â€ƒWe show that largeâ€‘language models often add `try/except`
> blocks that *silently swallow* errors.  Our ASTâ€‘based metric pipeline
> lets anyone quantify that risk across thousands of generated scripts
> in seconds.

---


## 1Â Â Scope of this study

Modern LLMs can write Python that â€œrunsâ€, but *how* it fails matters.
A **bare** `except:` or a blanket `exceptÂ Exception:` with no
reâ€‘raise can mask fatal bugs, leading to silent data corruption or
debugging nightmaresâ€”these are the **silentÂ killers**.

We collected **5 seeds Ã—Â 8 models Ã—Â 3 prompts** (easyÂ â†’Â hard rewrite
tasks) and asked:

* How often do models inject `try/except` at all?  
* Of those, how many are â€œbadâ€ under a strict reâ€‘raise rule?  
* Does difficulty exacerbate the problem?

The full paper is on my portfolio: 
[https://julianquick.com/ML/llm-audit.html](https://julianquick.com/ML/llm-audit.html).

---

## 2 Quick start

```
$ # generate example scripts to analyze
$ printf "try:\n    print(10 / 0)\nexcept:\n    pass\n" > example.py
$ silent-killers-audit example.py

âŒ example.py: 1 bad exception block(s)

```


### 2.1Â Â Generate metrics CSVs

```bash
python scripts/process_files.py --base-dir data/propagation_prompt
python scripts/process_files.py --base-dir data/calibration_prompt
python scripts/process_files.py --base-dir data/optimization_prompt
```

Each run creates

```
data/<prompt_dir>/
    llm_code_metrics.csv
    llm_response_metrics.csv
```

### 2.2Â Â Plots & summary tables

```bash
python scripts/post_processing.py --root data
```

Creates:

```
plots_grid_refactored/
    grid_status_3color.png
    grid_loc_continuous.png
    grid_bad_exception_rate.png
    grid_bad_exception_count.png
    bar_parsed_ok_by_difficulty.png
    summary_by_model.csv
    summary_by_difficulty.csv
```

<details>
<summary>Example output</summary>

| codeâ€‘status | badâ€‘rate heatmap |
|-------------|------------------|
| <img src="data/figures/grid_status_3color.png" width="380"> | <img src="data/figures/grid_bad_exception_rate.png" width="380"> |

</details>

### 2.3Â Â Library usage

```python
from silent_killers.metrics_definitions import code_metrics

python_code = "try:\n    1/0\nexcept Exception:\n    pass"
for metric in code_metrics(python_code):
    print(metric.name, metric.value)
```

### 2.4Â Â Use in pre-commit
```
- repo: https://github.com/kilojoules/silent-killers
  rev: v0.1.7
  hooks:
    - id: silent-killers-audit

```

---


## 3Â Â Repository layout

```
repo-root/
â”œâ”€ src/
â”‚   â””â”€ silent_killers/            â† reusable package
â”‚        â”œâ”€ __init__.py
â”‚        â””â”€ metrics_definitions.py     (AST visitors & regex metrics)
â”‚        â””â”€ llm_api.py                 (interface analysis with different LLM APIs)

â”œâ”€ tests/                          â† reusable package unit tests
â”‚   â””â”€ test_exception_labels.py
â”‚
â”œâ”€ scripts/                        â† analysis scripts
â”‚   â”œâ”€ process_files.py            (generates metrics CSVs)
â”‚   â””â”€ post_processing.py          (creates plots & summary tables)
â”‚   â””â”€ run_experiments.py          (runs the 3 prompts using the models in models.yaml)
â”‚
â”œâ”€ data/                           â† studyâ€‘specific artifacts
â”‚   â”œâ”€ calibration_prompt/         (easy rewrite task)
â”‚   â”œâ”€ propagation_prompt/         (medium rewrite task)
â”‚   â”œâ”€ optimization_prompt/        (hard rewrite task)
â”‚   â””â”€ figures/                    (output plots & visualizations)
â”‚ 
â”œâ”€ pyproject.toml
|
â””â”€ README.md
```


---

## 4Â Â Installation

```bash
git clone https://github.com/kilojoules/silent-killers.git
cd silent-killers
python -m pip install --upgrade pip
pip install -e .
```

or using the pypi distribution

```bash
pip install silent-killers
```


> **Requires PythonÂ â‰¥Â 3.10**  
> Runtime deps: `pandas`, `numpy`, `matplotlib`

---

## 5Â Â Metrics at a glance

| metric | description |
|--------|-------------|
| `exception_handling_blocks` | count of `except` clauses |
| `bad_exception_blocks` | bare `except:` **or** `except Exception:` *without* `raise` |
| `bad_exception_rate` | `bad / total`, 2Â dp |
| `uses_traceback` | calls `traceback.print_exc()` / `.format_exc()` |
| â€¦ | see `src/silent-killers/metrics_definitions.py` |

---

## 6Â Â Key pilot finding

> **When a model adds *any* error handling, 50â€“100Â % of those handlers
> are unsafe.**  
> Inclusive badâ€‘rates look tame (0Â â€“Â 0.6) but conditional badâ€‘rates
> (`only_with_try`) spike to **1.0** for several models on simple
> prompts.

> For these reasons, we recoomend using the silent-killers-audit tool in pre-commit workflows. 

---

## 7Â Â Development

```bash
ruff check .          # lint
pytest                # run unit tests
coverage run -m pytest && coverage html
```

CI runs on GitHubÂ Actions across PythonÂ 3.10 and 3.11 (see `.github/workflows/ci.yml`).

---

## 8Â Â Roadmap

* ğŸš§ dynamic execution traces (runtime errors, coverage)  
* ğŸš§ extend to other unsafe patterns
* ğŸš§ support more languages than Python

PRs & issues welcome!

---

## 9Â Â License & citation

MITÂ License.  
If you use the metrics or figures, please cite:

```bibtex
@misc{Quick2025SilentKillers,
  title  = {Silent Killers: An Exploratory Audit of Exceptionâ€‘Handling in LLMâ€‘Generated Python},
  author = {Julian Quick},
  year   = {2025},
  url    = {https://github.com/kilojoules/silent-killers}
}
```

*Happy auditingÂ â€“Â donâ€™t let silent errors slip through!*

